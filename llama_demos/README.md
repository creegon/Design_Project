# Llama æ°´å°æ¼”ç¤ºè„šæœ¬

æœ¬ç›®å½•åŒ…å«åŸºç¡€çš„Llamaæ°´å°ç”Ÿæˆå’Œæ£€æµ‹æ¼”ç¤ºè„šæœ¬ã€‚

## ğŸ“ æ–‡ä»¶åˆ—è¡¨

| æ–‡ä»¶ | åŠŸèƒ½ | ä½¿ç”¨éš¾åº¦ |
|------|------|----------|
| `llama_simple_example.py` | æœ€ç®€å•çš„å…¥é—¨ç¤ºä¾‹ | â­ |
| `llama_watermark_demo.py` | å®Œæ•´åŠŸèƒ½æ¼”ç¤º | â­â­ |
| `llama_interactive_demo.py` | äº¤äº’å¼å‘½ä»¤è¡Œç•Œé¢ | â­â­â­ |
| `llama_batch_test.py` | æ‰¹é‡å‚æ•°æµ‹è¯• | â­â­â­â­ |
| `model_config.json` | æ¨¡å‹å’ŒAPIé…ç½®æ–‡ä»¶ | â­â­ |
| `model_config_manager.py` | é…ç½®ç®¡ç†å·¥å…· | â­â­ |

## âš™ï¸ æ¨¡å‹é…ç½®

æ‰€æœ‰æ¨¡å‹å’ŒAPIæä¾›å•†é…ç½®éƒ½åœ¨ `model_config.json` ä¸­ç®¡ç†ã€‚

### æŸ¥çœ‹é…ç½®

```powershell
# æŸ¥çœ‹æ‰€æœ‰é…ç½®æ‘˜è¦
python model_config_manager.py --summary

# åˆ—å‡ºæ‰€æœ‰APIæä¾›å•†
python model_config_manager.py --list-providers

# åˆ—å‡ºæ‰€æœ‰æ¨¡å‹
python model_config_manager.py --list-models

# æŸ¥çœ‹ç‰¹å®šæ¨¡å‹è¯¦æƒ…
python model_config_manager.py --model deepseek-v3

# åˆ—å‡ºæ‰€æœ‰æ°´å°é…ç½®
python model_config_manager.py --list-watermark

# åˆ—å‡ºæ‰€æœ‰ç”Ÿæˆé…ç½®
python model_config_manager.py --list-generation
```

### é…ç½®æ–‡ä»¶ç»“æ„

`model_config.json` åŒ…å«ï¼š
- **api_providers**: APIæœåŠ¡æä¾›å•†é…ç½®ï¼ˆDeepSeek, SiliconFlow, HuggingFaceç­‰ï¼‰
- **models**: æ¨¡å‹é…ç½®ï¼ˆåŒ…æ‹¬æ ‡è¯†ç¬¦ã€ä»·æ ¼ã€ä¸Šä¸‹æ–‡é•¿åº¦ç­‰ï¼‰
- **watermark_configs**: æ°´å°å‚æ•°é¢„è®¾ï¼ˆdefault, strong, weak, balancedï¼‰
- **generation_configs**: ç”Ÿæˆå‚æ•°é¢„è®¾ï¼ˆdefault, creative, preciseï¼‰

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```powershell
pip install -r requirements_llama.txt
```

### 2. è¿è¡Œç®€å•ç¤ºä¾‹

```powershell
# ä½¿ç”¨é»˜è®¤æ¨¡å‹ (Llama 2 7B)
python llama_simple_example.py

# æŒ‡å®šå…¶ä»–æ¨¡å‹
python llama_simple_example.py meta-llama/Llama-2-13b-hf
```

### 3. ä½¿ç”¨å¯åŠ¨è„šæœ¬ï¼ˆæ¨èï¼‰

```powershell
.\run_llama_demo.ps1
```

## ğŸ“– ä½¿ç”¨è¯´æ˜

### llama_simple_example.py

æœ€ç®€å•çš„å…¥é—¨è„šæœ¬ï¼Œå±•ç¤ºå®Œæ•´çš„æ°´å°æµç¨‹ã€‚

```powershell
python llama_simple_example.py [æ¨¡å‹åç§°]
```

**è¾“å‡º**:
- ç”Ÿæˆå¸¦æ°´å°çš„æ–‡æœ¬
- æ£€æµ‹ç»“æœï¼ˆz-score, p-valueç­‰ï¼‰
- å¯¹æ¯”ä¸å¸¦æ°´å°çš„æ–‡æœ¬

### llama_watermark_demo.py

åŒ…å«å¤šä¸ªæµ‹è¯•æ¡ˆä¾‹çš„å®Œæ•´æ¼”ç¤ºã€‚

```powershell
python llama_watermark_demo.py [æ¨¡å‹åç§°]
```

**ç‰¹ç‚¹**:
- æä¾› `LlamaWatermarkDemo` ç±»
- å¯å¯¼å…¥åˆ°å…¶ä»–è„šæœ¬
- åŒ…å«å¤šä¸ªæµ‹è¯•æç¤ºè¯

### llama_interactive_demo.py

äº¤äº’å¼å‘½ä»¤è¡Œç•Œé¢ã€‚

```powershell
python llama_interactive_demo.py --model_name meta-llama/Llama-2-7b-hf --gamma 0.25 --delta 2.0
```

**åŠŸèƒ½**:
- èœå•é©±åŠ¨ç•Œé¢
- å®æ—¶ç”Ÿæˆå’Œæ£€æµ‹
- åŠ¨æ€ä¿®æ”¹å‚æ•°

### llama_batch_test.py

æ‰¹é‡æµ‹è¯•ä¸åŒå‚æ•°ç»„åˆã€‚

```powershell
python llama_batch_test.py [æ¨¡å‹åç§°]
```

**ç‰¹ç‚¹**:
- è‡ªåŠ¨æµ‹è¯•å¤šä¸ªæç¤ºè¯
- å‚æ•°ç½‘æ ¼æœç´¢
- ç”ŸæˆJSONç»“æœå’Œç»Ÿè®¡æŠ¥å‘Š

### llama_model_config.py

æ¨¡å‹é…ç½®ç®¡ç†å·¥å…·ã€‚

```powershell
# åˆ—å‡ºæ‰€æœ‰æ”¯æŒçš„æ¨¡å‹
python llama_model_config.py --list-models

# åˆ—å‡ºæ°´å°é…ç½®é¢„è®¾
python llama_model_config.py --list-configs
```

## âš™ï¸ é…ç½®æ–‡ä»¶

### requirements_llama.txt

Pythonä¾èµ–åŒ…åˆ—è¡¨ã€‚

### llama_config_example.json

é…ç½®ç¤ºä¾‹æ–‡ä»¶ï¼ŒåŒ…å«ï¼š
- æ¨¡å‹é…ç½®
- æ°´å°å‚æ•°
- ç”Ÿæˆå‚æ•°
- æ£€æµ‹å‚æ•°

### run_llama_demo.ps1

PowerShellå¯åŠ¨è„šæœ¬ï¼Œæä¾›ï¼š
- æ¨¡å‹é€‰æ‹©èœå•
- åŠŸèƒ½é€‰æ‹©
- ä¾èµ–å®‰è£…å‘å¯¼

## ğŸ¯ ä½¿ç”¨åœºæ™¯

| éœ€æ±‚ | æ¨èè„šæœ¬ |
|------|----------|
| å¿«é€Ÿæµ‹è¯• | `llama_simple_example.py` |
| å¤šæ¬¡å®éªŒ | `llama_interactive_demo.py` |
| ä»£ç é›†æˆ | `llama_watermark_demo.py` (å¯¼å…¥ç±») |
| å‚æ•°å¯¹æ¯” | `llama_batch_test.py` |
| æŸ¥çœ‹é…ç½® | `llama_model_config.py` |

## ğŸ“Š è¾“å‡ºç»“æœ

- **llama_test_results/**: æ‰¹é‡æµ‹è¯•ç»“æœç›®å½•
  - `batch_test_results_*.json`: å®Œæ•´å®éªŒç»“æœ
  - `summary_*.txt`: ç»Ÿè®¡æ‘˜è¦

## ğŸ’¡ ç¤ºä¾‹ç”¨æ³•

### å¯¼å…¥ç±»åˆ°è‡ªå·±çš„ä»£ç 

```python
from llama_watermark_demo import LlamaWatermarkDemo

# åˆå§‹åŒ–
demo = LlamaWatermarkDemo(
    model_name="meta-llama/Llama-2-7b-hf",
    gamma=0.25,
    delta=2.0
)

# ç”Ÿæˆ
text = demo.generate_with_watermark("Your prompt")

# æ£€æµ‹
result = demo.detect_watermark(text)
```

## ğŸ”§ å‚æ•°è¯´æ˜

### æ°´å°å‚æ•°
- `gamma`: ç»¿åå•æ¯”ä¾‹ (æ¨è: 0.25)
- `delta`: æ°´å°å¼ºåº¦ (æ¨è: 2.0)
- `seeding_scheme`: ç§å­æ–¹æ¡ˆ (æ¨è: "selfhash")

### ç”Ÿæˆå‚æ•°
- `max_new_tokens`: æœ€å¤§ç”Ÿæˆtokenæ•° (é»˜è®¤: 200)
- `temperature`: é‡‡æ ·æ¸©åº¦ (é»˜è®¤: 0.7)
- `top_p`: nucleusé‡‡æ · (é»˜è®¤: 0.9)

## ğŸ“š ç›¸å…³æ–‡æ¡£

- **è¯¦ç»†æŒ‡å—**: `../docs_llama/LLAMA_DEMO_README.md`
- **å¿«é€Ÿå‚è€ƒ**: `../docs_llama/QUICK_REFERENCE.md`
- **é¡¹ç›®æ€»ç»“**: `../docs_llama/PROJECT_SUMMARY.md`

## âš ï¸ æ³¨æ„äº‹é¡¹

1. é¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼ˆçº¦13GBï¼‰
2. æ¨èä½¿ç”¨GPUï¼ˆè‡³å°‘14GBæ˜¾å­˜ï¼‰
3. CPUæ¨¡å¼è¾ƒæ…¢ä½†å¯ç”¨
4. ç¡®ä¿å‚æ•°åœ¨ç”Ÿæˆå’Œæ£€æµ‹æ—¶ä¸€è‡´

---

**é»˜è®¤æ¨¡å‹**: Llama 2 7B (meta-llama/Llama-2-7b-hf)  
**æ¨èæ˜¾å­˜**: 14GB+
