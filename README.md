# Watermark System ‚Äì Project Navigation

This project implements a watermark generation, detection, and hybrid watermark experimental system based on large language models, supporting multiple models and API providers.

## üìÅ Directory Structure

```
lm-watermarking/
‚îú‚îÄ‚îÄ docs_llama/               # Chinese documentation & navigation üìÑ
‚îú‚îÄ‚îÄ hybrid_watermark/         # Hybrid watermark experimental system ‚≠ê
‚îÇ   ‚îú‚îÄ‚îÄ hybrid_watermark_experiment.py   (Core experiment)
‚îÇ   ‚îú‚îÄ‚îÄ hybrid_watermark_interactive.py  (‚≠ê Interactive experiment interface)
‚îÇ   ‚îú‚îÄ‚îÄ hybrid_watermark_analyzer.py     (Results analysis tool)
‚îÇ   ‚îú‚îÄ‚îÄ statistical_evaluation.py        (Statistical evaluation module)
‚îÇ   ‚îî‚îÄ‚îÄ README.md                        ‚≠ê Directory description
‚îú‚îÄ‚îÄ llama_demos/              # Basic watermark demo scripts üìÑ
‚îÇ   ‚îú‚îÄ‚îÄ llama_simple_example.py          (Introductory example)
‚îÇ   ‚îú‚îÄ‚îÄ llama_watermark_demo.py          (Full demo)
‚îÇ   ‚îú‚îÄ‚îÄ llama_interactive_demo.py        (Interactive interface)
‚îÇ   ‚îú‚îÄ‚îÄ llama_batch_test.py              (Batch testing)
‚îÇ   ‚îú‚îÄ‚îÄ model_config_manager.py          (‚≠ê Model configuration manager)
‚îÇ   ‚îú‚îÄ‚îÄ model_config.json                (‚≠ê Model configuration file)
‚îÇ   ‚îî‚îÄ‚îÄ README.md                        ‚≠ê Directory description
‚îú‚îÄ‚îÄ upstream/
‚îÇ   ‚îî‚îÄ‚îÄ lm_watermarking/      # Original lm-watermarking full source code üì¶
‚îÇ       ‚îú‚îÄ‚îÄ alternative_prf_schemes.py
‚îÇ       ‚îú‚îÄ‚îÄ experiments/
‚îÇ       ‚îú‚îÄ‚îÄ hf_hub_space_demo/
‚îÇ       ‚îú‚îÄ‚îÄ homoglyph_data/
‚îÇ       ‚îú‚îÄ‚îÄ watermark_processor.py
‚îÇ       ‚îú‚îÄ‚îÄ demo_watermark.py
‚îÇ       ‚îú‚îÄ‚îÄ requirements.txt / setup.cfg / pyproject.toml
‚îÇ       ‚îî‚îÄ‚îÄ watermark_reliability_release/ ‚Ä¶
‚îú‚îÄ‚îÄ extended_watermark_processor.py      # Custom extended processor (626 lines)
‚îú‚îÄ‚îÄ REPORT_LLAMAWATERMARK_LLAMA.md       # October 24 experiment report
‚îú‚îÄ‚îÄ SUMMARY.md                           # Project summary
‚îî‚îÄ‚îÄ IMPORT_FIX.md                        # Import fix notes
```

**üìÑ indicates that the directory already contains a README.md documentation file**  
**‚≠ê indicates important files or new features**  
**üì¶ indicates that the full upstream project is packaged inside a single directory**

> All upstream code is now consolidated under `upstream/lm_watermarking/`.  
> You can import modules using statements like  
> `from upstream.lm_watermarking import watermark_processor`.  
> Custom modules (with Chinese comments) are kept in separate subdirectories at the repository root.

## üöÄ Quick Start

### 1. Configure the Model (Required)


First `llama_demos/model_config.json`Ôºö

```json
{
  "api_providers": {
    "openai": {
      "api_key": "your-openai-api-key-or-env:OPENAI_API_KEY"
    },
    "deepseek": {
      "api_key": "env:DEEPSEEK_API_KEY",
      "api_base": "https://api.deepseek.com/v1"
    }
  },
  "models": {
    "llama-3.2-3b": {
      "model_identifier": "meta-llama/Llama-3.2-3B-Instruct",
      "nickname": "llama-3.2-3b",
      "api_provider": "deepseek"
    }
  }
}
```

### 2. Basic Watermarking

```powershell

cd llama_demos

python llama_simple_example.py llama-3.2-3b

.\run_llama_demo.ps1
```

> Tip: `llama_simple_example.py` and `llama_batch_test.py` use the **first positional argument** to specify the model nickname; there is no `--model` option.

### 3. Hybrid Watermark Experiments

```powershell
# Enter the experiment directory
cd hybrid_watermark

# Run the interactive interface (recommended)
python hybrid_watermark_interactive.py

# Or run the full experiment script
python hybrid_watermark_experiment.py


## üìö Experiment Types

### Hybrid Watermark Experiments (3 types)

| Experiment No. | Name | Description |
|----------------|------|-------------|
| **Experiment 1** | Hybrid Configuration Experiment | Segment-level / parameter-level hybrid watermarking |
| **Experiment 2** | Key Cross-Detection | Seed-mixing / key-sharing strategies |
| **Experiment 3** | Cross-Model Shared Key | Multi-model cooperative watermarking |

### Statistical Evaluation Experiments (4 types)

| Experiment No. | Name | Description |
|----------------|------|-------------|
| **Experiment 4** | Sliding-Window Detection | Analyze uniformity of watermark signal distribution |
| **Experiment 5** | Window Sensitivity Analysis | Determine optimal window size |
| **Experiment 6** | Minimum Detectable Length | Find minimum length required for reliable detection |
| **Experiment 7** | Full Statistical Evaluation | Perform all three statistical analyses |

## üéØ Usage Scenarios

### Scenario 1: Quick Watermark Function Test

```powershell
cd llama_demos
python llama_simple_example.py llama-3.2-3b
```

**Best for:** First-time users to understand core features

### Scenario 2: Interactive Experiment Research

```powershell
cd hybrid_watermark
python hybrid_watermark_interactive.py --model llama-3.2-3b
```

**Best for:** Researchers comparing multiple watermark schemes  
**Features:**  
- 7 experiment types (3 hybrid + 4 statistical)  
- Real-time visualization  
- Automatic result saving  

### Scenario 3: Batch Parameter Testing

```powershell
cd llama_demos
python llama_batch_test.py llama-3.2-3b
```

**Best for:** Systematic parameter comparison studies

### Scenario 4: Result Analysis

```powershell
cd hybrid_watermark
python hybrid_watermark_analyzer.py
```

**Best for:** Analyzing saved experiment outputs

## üí° Supported Models

### API Providers
- **OpenAI**: GPT series  
- **DeepSeek**: DeepSeek series, Llama series  
- **Local Models**: Loaded via HuggingFace Transformers  

### Recommended Model Configuration

```json
{
  "models": {
    "llama-3.2-3b": {
      "model_identifier": "meta-llama/Llama-3.2-3B-Instruct",
      "api_provider": "deepseek",
      "description": "Small and efficient; recommended for daily use"
    },
    "gpt-4o-mini": {
      "model_identifier": "gpt-4o-mini",
      "api_provider": "openai",
      "description": "High-quality outputs; good for comparison studies"
    }
  }
}
```

### Model Management

```powershell
# List all configured models
cd llama_demos
python -c "from model_config_manager import ModelConfigManager; mgr = ModelConfigManager(); print(mgr.list_model_names())"

# View model details
python -c "from model_config_manager import ModelConfigManager; mgr = ModelConfigManager(); print(mgr.get_model_info_by_nickname('llama-3.2-3b'))"
```

## üîß Installing Dependencies

```powershell
# Method 1: Basic dependencies
cd llama_demos
pip install -r requirements_llama.txt

# Method 2: Full dependencies (recommended)
cd ..
pip install -r requirements.txt

# Main packages:
# - torch >= 2.0.0
# - transformers >= 4.30.0
# - openai >= 1.0.0
# - scipy
# - matplotlib
# - numpy
# - tqdm
```

## ‚öôÔ∏è Environment Setup

### 1. API Key Configuration (recommended: environment variables)

```powershell
# Windows PowerShell
$env:OPENAI_API_KEY = "your-openai-api-key"
$env:DEEPSEEK_API_KEY = "your-deepseek-api-key"

# Or configure inside model_config.json
{
  "api_providers": {
    "openai": {
      "api_key": "env:OPENAI_API_KEY"
    }
  }
}
```

### 2. GPU Configuration (optional)

```python
python hybrid_watermark_interactive.py --device cuda
```

## üÜò Frequently Asked Questions

### Q1: How do I add a new model?

```json
{
  "models": {
    "my-model": {
      "model_identifier": "organization/model-name",
      "nickname": "my-model",
      "api_provider": "openai",
      "description": "My custom model"
    }
  }
}
```

### Q2: What if detection accuracy is low?

Try the following:

1. **Increase delta** (e.g. 2.0 ‚Üí 2.5) ‚Äî strengthens watermark signal  
2. **Lower gamma** (e.g. 0.5 ‚Üí 0.4) ‚Äî improves signal-to-noise ratio  
3. **Generate longer text** ‚Äî more statistical evidence  

### Q3: What is Z-score?

Z-score measures statistical significance:

- **Z = 3.0** ‚Üí 99.87% confidence (**recommended**)  
- **Z = 4.0** ‚Üí 99.997% confidence (too strict; deprecated)  
- **Z = 2.5** ‚Üí 99.38% confidence (lenient)

Formula:  
`Z = (observed_green - expected_green) / std_dev`

### Q4: How to choose Gamma and Delta?

| Scenario | Gamma | Delta | Notes |
|----------|--------|--------|------|
| Quality-first | 0.5 | 1.5‚Äì2.0 | More natural text |
| Balanced | 0.5 | 2.0 | **Recommended default** |
| Detection-first | 0.25 | 2.5‚Äì3.0 | Strong signal, possible text impact |

### Q5: List all models?

```powershell
python -c "from model_config_manager import ModelConfigManager; print('\n'.join(ModelConfigManager().list_model_names()))"
```

### Q6: Where are results saved?

`hybrid_watermark/hybrid_watermark_results/`

- JSON data files  
- PNG visualization files  

### Q7: How to analyze existing results?

```powershell
cd hybrid_watermark
python hybrid_watermark_analyzer.py
```

## üéì Core Features

### Base Functionality
- ‚úÖ Watermark generation & detection  
- ‚úÖ Multi-model support (local/API)  
- ‚úÖ Model configuration system  
- ‚úÖ Interactive UI  
- ‚úÖ Batch testing  

### Hybrid Watermark Experiments
- ‚úÖ Segment-level hybrid  
- ‚úÖ Parameter-grid hybrid  
- ‚úÖ Seed variants  
- ‚úÖ Key sharing  
- ‚úÖ Cross-model cooperation  

### Statistical Evaluation
- ‚úÖ Sliding-window detection  
- ‚úÖ Window sensitivity  
- ‚úÖ Minimum-length analysis  
- ‚úÖ Full statistical evaluation  


### Watermark Parameters

| Parameter | Description | Default | Recommended Range |
|-----------|-------------|----------|--------------------|
| **gamma** | Green-list ratio | 0.5 | 0.25‚Äì0.5 |
| **delta** | Logits bias strength | 2.0 | 1.5‚Äì3.0 |
| **hash_key** | PRF seed | 15485863 | Any integer |
| **z_threshold** | Detection threshold | 3.0 | 2.5‚Äì4.0 |

**Parameter Notes**:
- **gamma**: Controls the proportion of green tokens in the vocabulary; affects expected green-token rate  
- **delta**: Controls how strongly green tokens are boosted; affects actual green-token rate  
- **z_threshold**: Statistical significance threshold (optimized to 3.0 to improve detection rate)

## üìä Experiment Results

Results are generated in the following location:

### Results Directory
- `hybrid_watermark/hybrid_watermark_results/` ‚Äî all experiment outputs

### Output File Types

**JSON Files** ‚Äî full data logs

```
sliding_window_20251024_143022.json
window_sensitivity_20251024_143155.json
minimum_length_20251024_143340.json
complete_statistical_eval_20251024_143512.json
```

**PNG Format** - Charts
```
sliding_window_20251024_143022.png
window_sensitivity_20251024_143155.png
minimum_length_20251024_143340.png
```

### JSON Structure

Each experiment result contains:
- `experiment_type`: experiment type identifier  
- `prompt`: prompt used  
- `watermark_config`: watermark parameter configuration  
- `generated_texts`: generated texts with full content  
- `results`: statistical analysis results  
- `detailed_results`: detailed detection data  

### Visualization Analysis

All statistical evaluation experiments automatically generate matplotlib charts:
- Z-score distribution curve  
- Detection-rate trend plot  
- Green-token ratio analysis  
- Success/Failure scatter plot  


## ‚úÖ Project Features

### 1. Unified Model Management
- ‚úÖ Supports multiple API providers (OpenAI, DeepSeek, etc.)
- ‚úÖ Model nickname system for simplified usage
- ‚úÖ Secure API key management via environment variables
- ‚úÖ Unified configuration file `model_config.json`

### 2. Complete Experiment Framework
- ‚úÖ 3 hybrid watermarking schemes (configuration / key / cross-model)
- ‚úÖ 4 statistical evaluation methods (window / sensitivity / minimum length / comprehensive)
- ‚úÖ Interactive interface with real-time feedback
- ‚úÖ Automatic saving of JSON + PNG results

### 3. Optimized Detection Algorithm
- ‚úÖ Z-score threshold optimization (3.0 vs 4.0)
- ‚úÖ Improved detection sensitivity (accuracy from 40% ‚Üí nearly 100%)
- ‚úÖ Maintains low false-positive rate (<0.13%)

### 4. Visualization & Analysis
- ‚úÖ Automatic chart generation with matplotlib
- ‚úÖ Z-score distribution, detection rate, green-token ratio
- ‚úÖ Success/failure scatter plots
- ‚úÖ Cumulative detection-rate curves

### 5. Research Tools
- ‚úÖ Sliding-window analysis of watermark uniformity
- ‚úÖ Window-sensitivity analysis for optimal parameters
- ‚úÖ Minimum-length analysis for detection thresholds
- ‚úÖ Batch-experiment support for large-scale testing


## üìñ Command Quick Reference

```powershell
# 1. Configuration check
cd llama_demos
python -c "from model_config_manager import ModelConfigManager; ModelConfigManager().validate_config()"

# 2. Quick test
python llama_simple_example.py llama-3.2-3b

# 3. Interactive experiment (recommended)
cd ../hybrid_watermark
python hybrid_watermark_interactive.py --model llama-3.2-3b

# 4. Statistical evaluation (full workflow, including sliding window, etc.)
python statistical_evaluation.py --model llama-3.2-3b

# 5. Result analysis
python hybrid_watermark_analyzer.py

# 6. View help
python hybrid_watermark_interactive.py --help

```

## üîó Related Resources

- **Original Project**: [lm-watermarking](https://github.com/jwkirchenbauer/lm-watermarking)
- **Paper**: [A Watermark for Large Language Models](https://arxiv.org/abs/2301.10226)
- **Key Files**:
  - `extended_watermark_processor.py` ‚Äì Watermark processor (626 lines)
  - `hybrid_watermark_interactive.py` ‚Äì Interactive interface (1558 lines)
  - `model_config_manager.py` ‚Äì Model manager (443 lines)


## üìù Changelog

### Latest Version (2025-10-24)

**New Features**:
- ‚úÖ Statistical evaluation module (4 evaluation methods)
- ‚úÖ Z-score threshold optimization (3.0 replacing 4.0)
- ‚úÖ Model configuration management system
- ‚úÖ Complete JSON output (including generated text)
- ‚úÖ Automatic visualization chart generation

**Improvements**:
- ‚úÖ Detection accuracy significantly improved (40% ‚Üí nearly 100% @ 200 tokens)
- ‚úÖ Experiment consolidation (5 ‚Üí 3 hybrid experiments)
- ‚úÖ Interactive UI optimization (7 experiment types)

**Bug Fixes**:
- ‚úÖ `hash_key` parameter passing error
- ‚úÖ Overly strict Z-score threshold
- ‚úÖ Inconsistent visualization chart thresholds

---

**Created**: October 23, 2025  
**Last Updated**: October 24, 2025  
**Recommended Model**: Llama 3.2 3B Instruct (DeepSeek API)  
**Experiment Types**: 3 hybrid experiments + 4 statistical evaluations

